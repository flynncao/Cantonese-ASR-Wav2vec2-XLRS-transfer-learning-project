{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4379dba",
   "metadata": {},
   "source": [
    "\n",
    "A data collator class for CTC (Connectionist Temporal Classification) with padding functionality.\n",
    "This class handles the batching and padding of input features and labels for wav2vec2 model training.\n",
    "It processes audio features and their corresponding transcription labels, ensuring proper padding\n",
    "and tensor conversion.\n",
    "Args:\n",
    "\tprocessor (Wav2Vec2Processor): The wav2vec2 processor for handling inputs and labels\n",
    "\tpadding (Union[bool, str]): The padding strategy to use. Defaults to True.\n",
    "\tmax_length (Optional[int]): Maximum length for input features padding. Defaults to None.\n",
    "\tmax_length_labels (Optional[int]): Maximum length for labels padding. Defaults to None.\n",
    "\tpad_to_multiple_of (Optional[int]): Pad input features to be multiple of this value. Defaults to None.\n",
    "\tpad_to_multiple_of_labels (Optional[int]): Pad labels to be multiple of this value. Defaults to None.\n",
    "Methods:\n",
    "\t__call__(features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "\t\tProcesses a batch of features to create padded tensors suitable for model training.\n",
    "\t\tArgs:\n",
    "\t\t\tfeatures: List of dictionaries containing input values and labels\n",
    "\t\tReturns:\n",
    "\t\t\tDict containing padded input tensors and processed labels with -100 for padding tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c5bd447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import transformers\n",
    "from datasets import ClassLabel, load_dataset, load_metric, load_from_disk\n",
    "from transformers import (Trainer, TrainingArguments, Wav2Vec2CTCTokenizer,\n",
    "                          Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC,\n",
    "                          Wav2Vec2Processor)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b265da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(model='facebook/wav2vec2-large-xlsr-53', unfreeze=False, lr=0.0003, warmup=500, fff='c:\\\\Users\\\\westw\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v3f64c2e6cf0900acf9997538cf609a7651e8d62c7.json')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser() \n",
    "parser.add_argument('--model', type=str, default=\"facebook/wav2vec2-large-xlsr-53\")\n",
    "parser.add_argument('--unfreeze', action='store_true')\n",
    "parser.add_argument('--lr', type=float, default=3e-4)\n",
    "parser.add_argument('--warmup', type=float, default=500)\n",
    "parser.add_argument('-f', '--fff', help=\"dummy argument to avoid error in Jupyter\", default=\"dummy_value\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"args: {args}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc33a3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 从本地磁盘加载数据集 Load Cantonese language only \n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"zh-HK\", split=\"train\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"zh-HK\", split=\"test\")\n",
    "\n",
    "unused_cols = [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"]\n",
    "common_voice_train = common_voice_train.remove_columns(unused_cols)\n",
    "common_voice_test = common_voice_test.remove_columns(unused_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9edf162f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence', 'variant'],\n",
       "    num_rows: 5593\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe6719d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/8425 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8425/8425 [00:00<00:00, 328875.47 examples/s]\n",
      "Map: 100%|██████████| 5593/5593 [00:00<00:00, 302541.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "\n",
    "chars_to_ignore_regex = '[\\丶\\,\\?\\.\\!\\-\\;\\:\"\\“\\%\\‘\\”\\�\\．\\⋯\\！\\－\\：\\–\\。\\》\\,\\）\\,\\？\\；\\～\\~\\…\\︰\\，\\（\\」\\‧\\《\\﹔\\、\\—\\／\\,\\「\\﹖\\·\\']'\n",
    "\n",
    "import string\n",
    "def remove_special_characters(batch):\n",
    "    sen = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    if \"d\" in sen:\n",
    "        if len([c for c in sen if c in string.ascii_lowercase]) == 1:\n",
    "            sen = sen.replace(\"d\", \"啲\")\n",
    "    batch[\"sentence\"] = sen\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)\n",
    "\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"sentence\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names,)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names,)\n",
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
    "vocab_list = [char for char in vocab_list if not char.isascii()]\n",
    "vocab_list.append(\" \")\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "with open(\"vocab.json\", \"w\") as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44a272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init tokenizer\n",
    "\n",
    "# resamplers = {\n",
    "#     48000: torchaudio.transforms.Resample(48000, 16000),\n",
    "#     44100: torchaudio.transforms.Resample(44100, 16000),\n",
    "#     32000: torchaudio.transforms.Resample(32000, 16000), \n",
    "# }\n",
    "\n",
    "\n",
    "# def load_and_resample(batch):\n",
    "#     speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "#     batch[\"speech\"] = resamplers[sampling_rate](speech_array).squeeze().numpy()\n",
    "#     batch[\"sampling_rate\"] = 16_000\n",
    "#     batch[\"target_text\"] = batch[\"sentence\"]\n",
    "#     return batch\n",
    "\n",
    "# common_voice_train = common_voice_train.map(load_and_resample, remove_columns=common_voice_train.column_names,)\n",
    "# common_voice_test = common_voice_test.map(load_and_resample, remove_columns=common_voice_test.column_names,)\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "#     with processor.as_target_processor():\n",
    "#         batch[\"labels\"] = processor(batch[\"target_text\"]).input_ids\n",
    "#     return batch\n",
    "\n",
    "# def prepare_dataset_wav2vec2(batch):\n",
    "#     audio = batch[\"audio\"] # This is a dict: {'array': ..., 'sampling_rate': ...}\n",
    "#     # The processor handles both resampling (if needed) and feature extraction\n",
    "#     features = processor(\n",
    "#         audio[\"array\"],\n",
    "#         sampling_rate=audio[\"sampling_rate\"],\n",
    "#         text=batch[\"sentence\"]\n",
    "#     )\n",
    "#     batch[\"input_values\"] = features.input_values[0]\n",
    "#     with processor.as_target_processor():\n",
    "#         batch[\"labels\"] = processor(batch[\"sentence\"]).input_ids\n",
    "#     return batch\n",
    "\t\n",
    "\t\n",
    "# # set batch to false to let processor handle the batching\n",
    "# common_voice_train = common_voice_train.map(prepare_dataset_wav2vec2, remove_columns=common_voice_train.column_names, batch_size=-1, num_proc=10, batched=False,)\n",
    "# common_voice_test = common_voice_test.map(prepare_dataset_wav2vec2, remove_columns=common_voice_test.column_names, batch_size=-1, num_proc=10, batched=False,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c3fcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets and resampling, the modern way\n",
    "from datasets import Audio\n",
    "common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "common_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c789e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': 'F:\\\\hf_home\\\\datasets\\\\downloads\\\\extracted\\\\3ee5ffca136c1c2287060526e62cd5c3b2bdcbca5812d1065a9fab9ec1ecb669\\\\zh-HK_train_0/common_voice_zh-HK_22942304.mp3', 'audio': {'path': 'F:\\\\hf_home\\\\datasets\\\\downloads\\\\extracted\\\\3ee5ffca136c1c2287060526e62cd5c3b2bdcbca5812d1065a9fab9ec1ecb669\\\\zh-HK_train_0/common_voice_zh-HK_22942304.mp3', 'array': array([ 5.45696821e-12,  2.72848411e-12,  3.63797881e-12, ...,\n",
      "        1.48210138e-05,  9.73203896e-07, -4.09249424e-06]), 'sampling_rate': 16000}, 'sentence': '才能勇往直前 ', 'variant': ''}\n"
     ]
    }
   ],
   "source": [
    "# print sample rows from common_voice_train\n",
    "print(common_voice_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "401d6c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"Wav2Vec2Processor\",\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='', vocab_size=3653, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t3651: AddedToken(\"[UNK]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t3652: AddedToken(\"[PAD]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t3653: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3654: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Wav2Vec2Processor\"\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Define the prepare_dataset function (like your Whisper one) ---\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True,)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor.save_pretrained(\"./wav2vec2-large-xlsr-cantonese\")\n",
    "\n",
    "processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca1c53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=3): 100%|██████████| 8425/8425 [04:03<00:00, 34.64 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'path': 'F:\\\\hf_home\\\\datasets\\\\downloads\\\\extracted\\\\3ee5ffca136c1c2287060526e62cd5c3b2bdcbca5812d1065a9fab9ec1ecb669\\\\zh-HK_train_0/common_voice_zh-HK_22942304.mp3',\n",
       " 'audio': {'path': None,\n",
       "  'array': array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00, -3.05175781e-05]),\n",
       "  'sampling_rate': 16000},\n",
       " 'sentence': '才能勇往直前 ',\n",
       " 'variant': '',\n",
       " 'input_values': [0.00012427246838342398,\n",
       "  0.00012427243927959353,\n",
       "  0.00012427245383150876,\n",
       "  0.0001242725847987458,\n",
       "  0.00012427243927959353,\n",
       "  0.00012427259935066104,\n",
       "  0.0001242724247276783,\n",
       "  0.00012427239562384784,\n",
       "  0.00012427243927959353,\n",
       "  0.00012427264300640672,\n",
       "  0.00012427227920852602,\n",
       "  0.00012427257024683058,\n",
       "  0.0001242724829353392,\n",
       "  0.00012427200272213668,\n",
       "  0.0001242720609297976,\n",
       "  0.00012427179899532348,\n",
       "  0.00012427204637788236,\n",
       "  0.00012427147885318846,\n",
       "  0.0001242719154106453,\n",
       "  0.00012427163892425597,\n",
       "  0.0001242717116838321,\n",
       "  0.0001242700091097504,\n",
       "  0.00012426798639353365,\n",
       "  0.0001242697617271915,\n",
       "  0.00012427210458554327,\n",
       "  0.00012427332694642246,\n",
       "  0.00012427069304976612,\n",
       "  0.0001242685248143971,\n",
       "  0.00012427367619238794,\n",
       "  0.00012427412730176002,\n",
       "  0.00012426843750290573,\n",
       "  0.00012425449676811695,\n",
       "  0.0001242514990735799,\n",
       "  0.0001242619619006291,\n",
       "  0.00012426827743183821,\n",
       "  0.00012426773901097476,\n",
       "  0.00012425969180185348,\n",
       "  0.0001242545695276931,\n",
       "  0.0001242489233845845,\n",
       "  0.00012425311433617026,\n",
       "  0.00012426372268237174,\n",
       "  0.00012427338515408337,\n",
       "  0.00012426264584064484,\n",
       "  0.0001242121506948024,\n",
       "  0.0001242088619619608,\n",
       "  0.00012425618479028344,\n",
       "  0.00012428789341356605,\n",
       "  0.0001242668804479763,\n",
       "  0.00012420554412528872,\n",
       "  0.00012426695320755243,\n",
       "  0.00012431497452780604,\n",
       "  0.0001242766884388402,\n",
       "  0.00012414601224008948,\n",
       "  0.00012407211761455983,\n",
       "  0.00012418300320859998,\n",
       "  0.00012425864406395704,\n",
       "  0.00012423921725712717,\n",
       "  0.0001241444842889905,\n",
       "  0.0001240816491190344,\n",
       "  0.00012407870963215828,\n",
       "  0.0001240928249899298,\n",
       "  0.00012422920553945005,\n",
       "  0.0001243240258190781,\n",
       "  0.00012421667634043843,\n",
       "  0.0001238629047293216,\n",
       "  0.00012376470840536058,\n",
       "  0.00012415922537911683,\n",
       "  0.00012446906475815922,\n",
       "  0.0001243880979018286,\n",
       "  0.00012390443589538336,\n",
       "  0.0001240500423591584,\n",
       "  0.00012448559573385864,\n",
       "  0.00012432220682967454,\n",
       "  0.0001238151453435421,\n",
       "  0.00012348902237135917,\n",
       "  0.00012422126019373536,\n",
       "  0.0001247182080987841,\n",
       "  0.0001243327569682151,\n",
       "  0.00012368285388220102,\n",
       "  0.0001229618355864659,\n",
       "  0.0001229619374498725,\n",
       "  0.0001229530171258375,\n",
       "  0.00012364628491923213,\n",
       "  0.0001243106962647289,\n",
       "  0.0001238820404978469,\n",
       "  0.00012211060675326735,\n",
       "  0.0001210064729093574,\n",
       "  0.0001232821523444727,\n",
       "  0.00012542221520561725,\n",
       "  0.00012574064021464437,\n",
       "  0.00012301235983613878,\n",
       "  0.00012198646436445415,\n",
       "  0.00012503877223934978,\n",
       "  0.00012574903666973114,\n",
       "  0.0001227351021952927,\n",
       "  0.00011868277215398848,\n",
       "  0.00011913963680854067,\n",
       "  0.00012366472219582647,\n",
       "  0.00012702299864031374,\n",
       "  0.00012514839181676507,\n",
       "  0.00012137860176153481,\n",
       "  0.00011992215149803087,\n",
       "  0.0001178976017399691,\n",
       "  0.00011904900020454079,\n",
       "  0.00012461749429348856,\n",
       "  0.00013106162077747285,\n",
       "  0.00013479485642164946,\n",
       "  0.00013211531040724367,\n",
       "  0.00012621439236681908,\n",
       "  0.00012411312491167337,\n",
       "  0.00012536830035969615,\n",
       "  0.00012466007319744676,\n",
       "  0.0001235517702298239,\n",
       "  0.0001254036760656163,\n",
       "  0.00013171639875508845,\n",
       "  0.00013836644939146936,\n",
       "  0.00014187241322360933,\n",
       "  0.00013923883670940995,\n",
       "  0.00013179541565477848,\n",
       "  0.0001287108607357368,\n",
       "  0.00012534228153526783,\n",
       "  0.00011846637062262744,\n",
       "  0.00011507616000017151,\n",
       "  0.00011888460721820593,\n",
       "  0.0001275294489460066,\n",
       "  0.00013705015589948744,\n",
       "  0.00014162164006847888,\n",
       "  0.00013962810044176877,\n",
       "  0.00013838874292559922,\n",
       "  0.00013342358579393476,\n",
       "  0.00012584705837070942,\n",
       "  0.00012103479821234941,\n",
       "  0.00012474684626795352,\n",
       "  0.00013574236072599888,\n",
       "  0.0001341013121418655,\n",
       "  0.00012511243403423578,\n",
       "  0.00012295154738239944,\n",
       "  0.0001305745536228642,\n",
       "  0.0001362280163448304,\n",
       "  0.00012741604587063193,\n",
       "  0.00011593212548177689,\n",
       "  0.00011784212983911857,\n",
       "  0.00013184608542360365,\n",
       "  0.00013488488912116736,\n",
       "  0.00013042888895142823,\n",
       "  0.00012711140152532607,\n",
       "  0.00012032454833388329,\n",
       "  0.00011019278463209048,\n",
       "  9.81492776190862e-05,\n",
       "  0.00010461583588039503,\n",
       "  0.0001274836977245286,\n",
       "  0.00014783864025957882,\n",
       "  0.00014452631876338273,\n",
       "  0.0001261536672245711,\n",
       "  0.00012261414667591453,\n",
       "  0.00011962528515141457,\n",
       "  0.00010822315380210057,\n",
       "  9.937881986843422e-05,\n",
       "  0.00011188204371137545,\n",
       "  0.00013306550681591034,\n",
       "  0.00013700398267246783,\n",
       "  0.00012616060848813504,\n",
       "  0.00011256913421675563,\n",
       "  0.00011545462621143088,\n",
       "  0.00011365980753907934,\n",
       "  0.000105035345768556,\n",
       "  0.00010883989307330921,\n",
       "  0.00012567531666718423,\n",
       "  0.00014519307296723127,\n",
       "  0.00014350839774124324,\n",
       "  0.0001379696186631918,\n",
       "  0.0001380563626298681,\n",
       "  0.00014037366781849414,\n",
       "  0.00013013686111662537,\n",
       "  0.00010989155998686329,\n",
       "  0.00011323523358441889,\n",
       "  0.00012409286864567548,\n",
       "  0.00012358489038888365,\n",
       "  0.00010677031241357327,\n",
       "  0.00010267958714393899,\n",
       "  0.00011759489279938862,\n",
       "  0.00012170719128334895,\n",
       "  0.00012154653813922778,\n",
       "  0.00012581404007505625,\n",
       "  0.0001475128228776157,\n",
       "  0.00015880001592449844,\n",
       "  0.0001467618130845949,\n",
       "  0.0001302527671214193,\n",
       "  0.0001216532036778517,\n",
       "  0.00012554097338579595,\n",
       "  0.00011258848826400936,\n",
       "  0.00010644138092175126,\n",
       "  0.0001231256901519373,\n",
       "  0.00014519102114718407,\n",
       "  0.00014815852046012878,\n",
       "  0.00012022193550365046,\n",
       "  0.00010157911310670897,\n",
       "  0.0001001422424451448,\n",
       "  0.00010253776417812333,\n",
       "  9.384826989844441e-05,\n",
       "  0.00010335483239032328,\n",
       "  0.00014358252519741654,\n",
       "  0.00016277391114272177,\n",
       "  0.00014806109538767487,\n",
       "  0.00011305203224765137,\n",
       "  9.4983923190739e-05,\n",
       "  9.373982902616262e-05,\n",
       "  9.552357369102538e-05,\n",
       "  0.00010259254486300051,\n",
       "  0.00011155938409501687,\n",
       "  0.00013187633885536343,\n",
       "  0.0001282054145121947,\n",
       "  0.0001021009375108406,\n",
       "  9.205738751916215e-05,\n",
       "  0.00010704668238759041,\n",
       "  0.0001226174208568409,\n",
       "  0.00011287331290077418,\n",
       "  9.94429356069304e-05,\n",
       "  0.00010093831951962784,\n",
       "  0.00012208167754579335,\n",
       "  0.00012817942479159683,\n",
       "  0.00011032847396563739,\n",
       "  9.723316907184198e-05,\n",
       "  9.072585817193612e-05,\n",
       "  9.746681462274864e-05,\n",
       "  0.00010825708886841312,\n",
       "  0.00012642130604945123,\n",
       "  0.00014896743232384324,\n",
       "  0.00015595005243085325,\n",
       "  0.00014866293349768966,\n",
       "  0.00012761580001097172,\n",
       "  0.00010615794599289075,\n",
       "  9.840709390118718e-05,\n",
       "  9.897406562231481e-05,\n",
       "  8.976781100500375e-05,\n",
       "  9.194965241476893e-05,\n",
       "  0.00012038271961500868,\n",
       "  0.00014736886078026146,\n",
       "  0.00016296928515657783,\n",
       "  0.00015737581998109818,\n",
       "  0.00014333383296616375,\n",
       "  0.00013709334598388523,\n",
       "  0.00013689666229765862,\n",
       "  0.00012593966675922275,\n",
       "  0.00010512815060792491,\n",
       "  0.000105076796899084,\n",
       "  0.00014025994460098445,\n",
       "  0.00017306501104030758,\n",
       "  0.0001782159524736926,\n",
       "  0.00016407914517913014,\n",
       "  0.00014300821931101382,\n",
       "  0.0001343952026218176,\n",
       "  0.00012841956049669534,\n",
       "  0.0001291283406317234,\n",
       "  0.00015242469089571387,\n",
       "  0.00016505482199136168,\n",
       "  0.00013912259601056576,\n",
       "  0.00010626891889842227,\n",
       "  9.626006794860587e-05,\n",
       "  0.00012285294360481203,\n",
       "  0.00015105475904420018,\n",
       "  0.00014730375551152974,\n",
       "  0.00012549191887956113,\n",
       "  0.0001345626951660961,\n",
       "  0.00014527332677971572,\n",
       "  0.00015331839676946402,\n",
       "  0.00012863626761827618,\n",
       "  0.00010324372124159709,\n",
       "  0.0001162763437605463,\n",
       "  0.00012003788287984207,\n",
       "  0.00012302571849431843,\n",
       "  0.00012520547898020595,\n",
       "  0.0001428506220690906,\n",
       "  0.00012472212256398052,\n",
       "  0.00010473046131664887,\n",
       "  9.98535833787173e-05,\n",
       "  0.00010643484711181372,\n",
       "  0.00010848138481378555,\n",
       "  0.00010927748371614143,\n",
       "  0.00011736442684195936,\n",
       "  0.00011875102063640952,\n",
       "  0.0001489377609686926,\n",
       "  0.00017326489614788443,\n",
       "  0.00017044016567524523,\n",
       "  0.0001849959371611476,\n",
       "  0.00015455730317626148,\n",
       "  0.00010313074744772166,\n",
       "  8.34591846796684e-05,\n",
       "  8.075866935541853e-05,\n",
       "  7.521743100369349e-05,\n",
       "  0.00011108537000836805,\n",
       "  0.0001315657573286444,\n",
       "  0.0001340189774055034,\n",
       "  0.0001489731075707823,\n",
       "  0.00010736627882579342,\n",
       "  0.00011179062857991084,\n",
       "  0.00018250571156386286,\n",
       "  0.00015643560618627816,\n",
       "  7.642251148354262e-05,\n",
       "  0.00010997683421010152,\n",
       "  9.733634942676872e-05,\n",
       "  0.00010127042332896963,\n",
       "  9.660608338890597e-05,\n",
       "  6.254095205804333e-05,\n",
       "  6.484128243755549e-05,\n",
       "  0.00014189346984494478,\n",
       "  0.0001631217310205102,\n",
       "  7.48348975321278e-05,\n",
       "  8.707221422810107e-05,\n",
       "  0.00012183800572529435,\n",
       "  0.00011330671259202063,\n",
       "  0.0001668858458288014,\n",
       "  0.00011847986752400175,\n",
       "  0.00013277870311867446,\n",
       "  0.00021156507136765867,\n",
       "  0.0001564368576509878,\n",
       "  0.0001010781925288029,\n",
       "  7.753262616461143e-05,\n",
       "  4.709160202764906e-05,\n",
       "  4.845269722864032e-05,\n",
       "  0.00013471287093125284,\n",
       "  6.332677003229037e-05,\n",
       "  9.313143527833745e-05,\n",
       "  0.00017031197785399854,\n",
       "  8.793070446699858e-05,\n",
       "  -1.8597269445308484e-05,\n",
       "  6.369356560753658e-05,\n",
       "  6.450612272601575e-05,\n",
       "  3.1880004826234654e-05,\n",
       "  0.0001412276178598404,\n",
       "  0.0001615115616004914,\n",
       "  9.079783922061324e-05,\n",
       "  0.00015689949213992804,\n",
       "  0.0001328306389041245,\n",
       "  6.808857870055363e-05,\n",
       "  0.00013329731882549822,\n",
       "  0.00016088644042611122,\n",
       "  0.0002184373588534072,\n",
       "  0.00014305528020486236,\n",
       "  0.00015097646974027157,\n",
       "  0.00013497163308784366,\n",
       "  8.370612340513617e-05,\n",
       "  3.923730400856584e-05,\n",
       "  -5.011372559238225e-05,\n",
       "  4.965630796505138e-05,\n",
       "  0.00019541113579180092,\n",
       "  0.00022967647237237543,\n",
       "  0.0003502730978652835,\n",
       "  0.0002104325540130958,\n",
       "  0.00013435471919365227,\n",
       "  0.00010407392983324826,\n",
       "  4.9349633627571166e-05,\n",
       "  0.00013176487118471414,\n",
       "  7.83202049206011e-05,\n",
       "  0.00012251535372342914,\n",
       "  0.0001711162185529247,\n",
       "  7.522560918005183e-05,\n",
       "  0.0002061925333691761,\n",
       "  0.00012680466170422733,\n",
       "  2.750482235569507e-05,\n",
       "  0.00010499796189833432,\n",
       "  8.85922199813649e-05,\n",
       "  0.0002589475770946592,\n",
       "  4.3366024328861386e-05,\n",
       "  8.776970935286954e-05,\n",
       "  0.00010329842916689813,\n",
       "  6.596687853743788e-06,\n",
       "  0.00018975159036926925,\n",
       "  7.118201756384224e-05,\n",
       "  0.00016035998123697937,\n",
       "  0.0001516527554485947,\n",
       "  6.163264970382443e-06,\n",
       "  0.00016057697939686477,\n",
       "  -2.380676414759364e-05,\n",
       "  0.00012892403174191713,\n",
       "  0.00020799256162717938,\n",
       "  0.00016777837299741805,\n",
       "  0.00021349883172661066,\n",
       "  9.02981628314592e-05,\n",
       "  0.00019100826466456056,\n",
       "  0.00011568504851311445,\n",
       "  0.00018633664876688272,\n",
       "  0.00043992375140078366,\n",
       "  0.0004512938321568072,\n",
       "  0.00044352668919600546,\n",
       "  0.000426426442572847,\n",
       "  0.00018582792836241424,\n",
       "  8.96793935680762e-05,\n",
       "  -7.911132706794888e-05,\n",
       "  0.00019098457414656878,\n",
       "  0.0005007185973227024,\n",
       "  0.0005017059738747776,\n",
       "  0.00046175342868082225,\n",
       "  0.00018139035091735423,\n",
       "  8.741430065128952e-05,\n",
       "  0.0001184422944788821,\n",
       "  -1.1681667274388019e-05,\n",
       "  -5.24551905982662e-05,\n",
       "  -0.00015745543350931257,\n",
       "  7.181705586845055e-05,\n",
       "  0.0002667276421561837,\n",
       "  0.0003286571300122887,\n",
       "  0.0004607180308084935,\n",
       "  0.0004017248284071684,\n",
       "  0.000377221847884357,\n",
       "  -8.254603744717315e-05,\n",
       "  -0.0003102249756921083,\n",
       "  -2.480224247847218e-05,\n",
       "  0.00014121562708169222,\n",
       "  0.00021161194308660924,\n",
       "  0.0005597686395049095,\n",
       "  0.0006822354625910521,\n",
       "  0.0005346684483811259,\n",
       "  0.00038444818346761167,\n",
       "  6.543567724293098e-05,\n",
       "  -0.0003610260901041329,\n",
       "  -0.00020999094704166055,\n",
       "  -5.802381201647222e-05,\n",
       "  -0.0001819467288441956,\n",
       "  -0.0002403092512395233,\n",
       "  -0.0001829895336413756,\n",
       "  -0.00024045293685048819,\n",
       "  -0.00023708386288490146,\n",
       "  -0.00012948182120453566,\n",
       "  -0.00019186853023711592,\n",
       "  0.00010971462324960157,\n",
       "  0.00020125016453675926,\n",
       "  0.0001680795248830691,\n",
       "  0.00034483513445593417,\n",
       "  0.0001913413143483922,\n",
       "  -4.960250953445211e-05,\n",
       "  -0.00018056850240100175,\n",
       "  -0.0003361548588145524,\n",
       "  -0.00023010667064227164,\n",
       "  0.0001346858189208433,\n",
       "  0.0004843032220378518,\n",
       "  0.0003023039025720209,\n",
       "  0.00020745696383528411,\n",
       "  -0.00034991977736353874,\n",
       "  -0.0008699348545633256,\n",
       "  -0.0007325304904952645,\n",
       "  -0.0004543439135886729,\n",
       "  -0.0002782067167572677,\n",
       "  7.380346505669877e-05,\n",
       "  0.000533790560439229,\n",
       "  0.0006388113833963871,\n",
       "  0.0006675201002508402,\n",
       "  0.00015753143816255033,\n",
       "  -0.00033118121791630983,\n",
       "  -0.0006488062208518386,\n",
       "  -0.0008681176695972681,\n",
       "  -0.0006202028598636389,\n",
       "  -0.00017718199524097145,\n",
       "  0.00022551201982423663,\n",
       "  0.0003991435223724693,\n",
       "  0.000598520680796355,\n",
       "  0.0006371812196448445,\n",
       "  0.000392166810343042,\n",
       "  0.0003322998236399144,\n",
       "  -1.7578280676389113e-05,\n",
       "  2.398274591541849e-05,\n",
       "  0.0001966411800822243,\n",
       "  0.00035396323073655367,\n",
       "  0.00035257916897535324,\n",
       "  0.0005537986871786416,\n",
       "  0.0007119258516468108,\n",
       "  0.0006471058586612344,\n",
       "  0.00042191101238131523,\n",
       "  -4.49576873506885e-05,\n",
       "  0.00019349278591107577,\n",
       "  0.00010430759721202776,\n",
       "  0.00041255648829974234,\n",
       "  0.0006142109050415456,\n",
       "  0.00019149457511957735,\n",
       "  7.217799429781735e-05,\n",
       "  -0.00013623220729641616,\n",
       "  -0.00031548080733045936,\n",
       "  1.3028501598455478e-05,\n",
       "  0.0021261125802993774,\n",
       "  0.003142220200970769,\n",
       "  0.0018732903990894556,\n",
       "  0.0024540575686842203,\n",
       "  0.0038851983845233917,\n",
       "  0.004711830988526344,\n",
       "  0.004819422960281372,\n",
       "  0.0042488472536206245,\n",
       "  0.002086266875267029,\n",
       "  -0.0006331909680739045,\n",
       "  -0.002060147002339363,\n",
       "  -0.002574342070147395,\n",
       "  -0.001538289594464004,\n",
       "  0.000799922680016607,\n",
       "  0.0026167714968323708,\n",
       "  0.002803244162350893,\n",
       "  0.001747424597851932,\n",
       "  -0.0007886327803134918,\n",
       "  -0.0031446758657693863,\n",
       "  -0.004842188209295273,\n",
       "  -0.005680219270288944,\n",
       "  -0.004824989940971136,\n",
       "  -0.002882178872823715,\n",
       "  -0.0005244977073743939,\n",
       "  -0.00015064446779433638,\n",
       "  -0.0009993222774937749,\n",
       "  -0.0016148065915331244,\n",
       "  -0.0025715758092701435,\n",
       "  -0.003078232519328594,\n",
       "  -0.0037387043703347445,\n",
       "  -0.00342757860198617,\n",
       "  -0.0029588525649160147,\n",
       "  -0.002384172286838293,\n",
       "  -0.0013896202435716987,\n",
       "  -0.001526881824247539,\n",
       "  -0.0018842930439859629,\n",
       "  -0.002616163343191147,\n",
       "  -0.0030751549638807774,\n",
       "  -0.0024216105230152607,\n",
       "  -0.0014090477488934994,\n",
       "  -0.0010663177818059921,\n",
       "  -0.0013988774735480547,\n",
       "  -0.0013452342245727777,\n",
       "  -0.000479245965834707,\n",
       "  0.00024606435908935964,\n",
       "  0.0003708830918185413,\n",
       "  7.660164556000382e-05,\n",
       "  0.00040244549745693803,\n",
       "  0.0010688151232898235,\n",
       "  0.0010041516507044435,\n",
       "  0.0008255491848103702,\n",
       "  0.0009130705147981644,\n",
       "  0.0006303610862232745,\n",
       "  0.0007835221476852894,\n",
       "  0.001956610009074211,\n",
       "  0.0034002182073891163,\n",
       "  0.004041443113237619,\n",
       "  0.003631460713222623,\n",
       "  0.002079489640891552,\n",
       "  -0.0001084552495740354,\n",
       "  -0.0018987244693562388,\n",
       "  -0.002893481170758605,\n",
       "  -0.0022940088529139757,\n",
       "  -0.001029068836942315,\n",
       "  1.2021740985801443e-05,\n",
       "  7.682709110667929e-05,\n",
       "  -0.0007505653193220496,\n",
       "  -0.0016300845891237259,\n",
       "  -0.0020505713764578104,\n",
       "  -0.0012306571006774902,\n",
       "  -0.00018911139341071248,\n",
       "  0.0003886751946993172,\n",
       "  0.0013172804610803723,\n",
       "  0.002192102838307619,\n",
       "  0.0030712694860994816,\n",
       "  0.003320181742310524,\n",
       "  0.0029256383422762156,\n",
       "  0.0022223421838134527,\n",
       "  0.0012183069484308362,\n",
       "  0.0008139517158269882,\n",
       "  0.001016251859255135,\n",
       "  0.001431069104000926,\n",
       "  0.0018198878969997168,\n",
       "  0.0012110392563045025,\n",
       "  -5.999525455990806e-05,\n",
       "  -0.00077818613499403,\n",
       "  -0.0011785191018134356,\n",
       "  -0.00092904461780563,\n",
       "  -0.000567490526009351,\n",
       "  0.00019617406360339373,\n",
       "  0.0006460906006395817,\n",
       "  0.00029417016776278615,\n",
       "  0.0006502351607196033,\n",
       "  0.001286409329622984,\n",
       "  0.0016037770546972752,\n",
       "  0.0012442718725651503,\n",
       "  0.0004174525092821568,\n",
       "  9.359648061035841e-07,\n",
       "  -4.185611032880843e-06,\n",
       "  0.0007743256282992661,\n",
       "  0.0018676754552870989,\n",
       "  0.0022250243928283453,\n",
       "  0.0019531650468707085,\n",
       "  0.001242948928847909,\n",
       "  -6.847507756901905e-05,\n",
       "  -0.0011910131433978677,\n",
       "  -0.0022276942618191242,\n",
       "  -0.0026496571954339743,\n",
       "  -0.0020859921351075172,\n",
       "  -0.000819315027911216,\n",
       "  0.0011582548031583428,\n",
       "  0.0018617239547893405,\n",
       "  0.00161604187451303,\n",
       "  6.058011422283016e-05,\n",
       "  -0.0016848125960677862,\n",
       "  -0.002980097895488143,\n",
       "  -0.004263159818947315,\n",
       "  -0.004156399052590132,\n",
       "  -0.0031859714072197676,\n",
       "  -0.0014227350475266576,\n",
       "  -7.906334212748334e-05,\n",
       "  0.0005262048216536641,\n",
       "  -2.409712760709226e-05,\n",
       "  -0.0025935207959264517,\n",
       "  -0.005328754894435406,\n",
       "  -0.00629766471683979,\n",
       "  -0.004982675425708294,\n",
       "  -0.003369611455127597,\n",
       "  -0.0021732847671955824,\n",
       "  -0.0004719471908174455,\n",
       "  0.001229343586601317,\n",
       "  0.0015687637496739626,\n",
       "  0.0009134073043242097,\n",
       "  0.0003464823239482939,\n",
       "  0.0001097473650588654,\n",
       "  -0.00012809407780878246,\n",
       "  -0.00034564585075713694,\n",
       "  0.00134721165522933,\n",
       "  0.003633267479017377,\n",
       "  0.004903715569525957,\n",
       "  0.004290891345590353,\n",
       "  0.0027310382574796677,\n",
       "  0.001661369577050209,\n",
       "  -5.1742146752076223e-05,\n",
       "  -0.00136667734477669,\n",
       "  -0.0015740972012281418,\n",
       "  -0.0003538804885465652,\n",
       "  0.0013761450536549091,\n",
       "  0.0018311806488782167,\n",
       "  0.0016254233196377754,\n",
       "  0.0010520053328946233,\n",
       "  0.00015812116907909513,\n",
       "  -1.95473858184414e-05,\n",
       "  0.00024685374228283763,\n",
       "  0.0007341217715293169,\n",
       "  0.0015369035536423326,\n",
       "  0.0018778701778501272,\n",
       "  0.0020817520562559366,\n",
       "  0.001667149132117629,\n",
       "  0.0012298075016587973,\n",
       "  0.0013106592232361436,\n",
       "  0.00131630664691329,\n",
       "  0.0017401304794475436,\n",
       "  0.0009178633335977793,\n",
       "  1.3187425793148577e-05,\n",
       "  -1.527860513306223e-05,\n",
       "  -0.00010504691454116255,\n",
       "  2.284711445099674e-05,\n",
       "  -2.8827918868046254e-05,\n",
       "  0.0008146066684275866,\n",
       "  0.0017973792273551226,\n",
       "  0.0017925443826243281,\n",
       "  0.0013161305105313659,\n",
       "  0.0006656544865109026,\n",
       "  -0.00014837287017144263,\n",
       "  -0.0012562080519273877,\n",
       "  -0.0013235382502898574,\n",
       "  0.0003689492295961827,\n",
       "  0.0022710261400789022,\n",
       "  0.0035184172447770834,\n",
       "  0.004091182723641396,\n",
       "  0.003558650380000472,\n",
       "  0.0022373294923454523,\n",
       "  0.0007650365587323904,\n",
       "  -0.00019947384134866297,\n",
       "  0.00023208516358863562,\n",
       "  0.0018755723722279072,\n",
       "  0.003014195244759321,\n",
       "  0.002802319824695587,\n",
       "  0.0024019121192395687,\n",
       "  0.002030561910942197,\n",
       "  0.0009555451106280088,\n",
       "  -0.0006301263347268105,\n",
       "  -0.0018959782319143414,\n",
       "  -0.0019189907470718026,\n",
       "  -0.0012214519083499908,\n",
       "  -0.0005090010236017406,\n",
       "  9.417096589459106e-05,\n",
       "  0.0004924564273096621,\n",
       "  0.0006793505162931979,\n",
       "  -0.0004399099852889776,\n",
       "  -0.0019353859825059772,\n",
       "  -0.002544916234910488,\n",
       "  -0.002058646874502301,\n",
       "  -0.00024186768860090524,\n",
       "  0.000772387720644474,\n",
       "  0.00022287476167548448,\n",
       "  -0.0006901647429913282,\n",
       "  -0.001172461430542171,\n",
       "  -0.0018846533494070172,\n",
       "  -0.002870482625439763,\n",
       "  -0.0026456634514033794,\n",
       "  -0.0020256871357560158,\n",
       "  -0.0025252411141991615,\n",
       "  -0.003654851345345378,\n",
       "  -0.004172390326857567,\n",
       "  -0.003257703734561801,\n",
       "  -0.0008997241966426373,\n",
       "  0.0009012796799652278,\n",
       "  0.001772983348928392,\n",
       "  0.0020511767361313105,\n",
       "  0.0013567155692726374,\n",
       "  0.0004935271572321653,\n",
       "  -0.0003949501260649413,\n",
       "  -0.00020782598585356027,\n",
       "  0.000802665192168206,\n",
       "  0.0019067125394940376,\n",
       "  0.002888391027227044,\n",
       "  0.00277294241823256,\n",
       "  0.002817840315401554,\n",
       "  0.0032076076604425907,\n",
       "  0.0030710299033671618,\n",
       "  0.0028308311011642218,\n",
       "  0.002648120280355215,\n",
       "  0.0034718720708042383,\n",
       "  0.004378253594040871,\n",
       "  0.004156940616667271,\n",
       "  0.004023608285933733,\n",
       "  0.003446136135607958,\n",
       "  0.0027721591759473085,\n",
       "  0.0023458395153284073,\n",
       "  0.0017963130958378315,\n",
       "  0.0012809702893719077,\n",
       "  0.0011045889696106315,\n",
       "  0.0016380812739953399,\n",
       "  0.0017157811671495438,\n",
       "  0.0015449469210579991,\n",
       "  0.0014653928810730577,\n",
       "  0.0014560673153027892,\n",
       "  0.0006746133440174162,\n",
       "  -0.0004973112372681499,\n",
       "  -0.0017808456905186176,\n",
       "  -0.0032987748272717,\n",
       "  -0.004168386105448008,\n",
       "  -0.005092718172818422,\n",
       "  -0.0047035361640155315,\n",
       "  -0.003963190596550703,\n",
       "  -0.0032098356168717146,\n",
       "  -0.002517649671062827,\n",
       "  -0.0017540020635351539,\n",
       "  -0.0002934012154582888,\n",
       "  -0.0005495160003192723,\n",
       "  -0.0014613392995670438,\n",
       "  -0.0030921397265046835,\n",
       "  -0.004168998450040817,\n",
       "  -0.004103187937289476,\n",
       "  -0.0038177527021616697,\n",
       "  -0.0033165246713906527,\n",
       "  -0.002863748464733362,\n",
       "  -0.0015587267698720098,\n",
       "  -0.0016822549514472485,\n",
       "  -0.0022441470064222813,\n",
       "  -0.0023011001758277416,\n",
       "  -0.002001302083954215,\n",
       "  -0.0010166835272684693,\n",
       "  -0.0007153291953727603,\n",
       "  -0.0005741134518757463,\n",
       "  -0.000666604086291045,\n",
       "  -0.0002980534336529672,\n",
       "  -0.0001243793813046068,\n",
       "  -0.0008157992851920426,\n",
       "  -0.0010228564497083426,\n",
       "  -0.0010798366274684668,\n",
       "  -0.000745169585570693,\n",
       "  0.0007799615850672126,\n",
       "  0.002733910223469138,\n",
       "  0.003989510703831911,\n",
       "  0.0040957629680633545,\n",
       "  0.0037608728744089603,\n",
       "  0.003118745516985655,\n",
       "  0.0018520023440942168,\n",
       "  0.0007768375799059868,\n",
       "  0.00025445001665502787,\n",
       "  0.0009512122487649322,\n",
       "  0.0018275605980306864,\n",
       "  0.001339781447313726,\n",
       "  0.00020999222761020064,\n",
       "  -0.0009887422202154994,\n",
       "  -0.001858926028944552,\n",
       "  -0.002378822537139058,\n",
       "  -0.0022857121657580137,\n",
       "  -0.0011350774439051747,\n",
       "  0.0003841242869384587,\n",
       "  0.0014914784114807844,\n",
       "  0.0018836628878489137,\n",
       "  0.001672629383392632,\n",
       "  0.0011876759817823768,\n",
       "  0.0003978883323725313,\n",
       "  2.930531081801746e-05,\n",
       "  0.0001523823302704841,\n",
       "  0.0007820983300916851,\n",
       "  0.0012965732021257281,\n",
       "  0.00023991135822143406,\n",
       "  -0.0006793872453272343,\n",
       "  -0.0010740896686911583,\n",
       "  -0.0010421796469017863,\n",
       "  0.00036029270268045366,\n",
       "  0.002442377619445324,\n",
       "  0.004125847481191158,\n",
       "  0.00412343442440033,\n",
       "  0.0027654136065393686,\n",
       "  0.0010370969539508224,\n",
       "  -0.0002340325590921566,\n",
       "  -0.0003663855604827404,\n",
       "  -3.1682680855738e-05,\n",
       "  0.0005877390503883362,\n",
       "  0.001985521987080574,\n",
       "  0.003256120951846242,\n",
       "  0.0033054365776479244,\n",
       "  0.002021270338445902,\n",
       "  0.0006925863563083112,\n",
       "  0.0002984335005749017,\n",
       "  0.0005508868489414454,\n",
       "  0.0016779802972450852,\n",
       "  0.0028114034794270992,\n",
       "  0.0033101763110607862,\n",
       "  0.0035596962552517653,\n",
       "  0.0034037595614790916,\n",
       "  0.0027207606472074986,\n",
       "  0.0018143539782613516,\n",
       "  0.001080045709386468,\n",
       "  0.0011117862304672599,\n",
       "  0.0015678441850468516,\n",
       "  0.001719599007628858,\n",
       "  0.0009374722721986473,\n",
       "  -0.0007041774806566536,\n",
       "  -0.0019240380497649312,\n",
       "  -0.003858419833704829,\n",
       "  -0.004400560632348061,\n",
       "  -0.0026969239115715027,\n",
       "  -0.0013296586694195867,\n",
       "  0.0003172573633491993,\n",
       "  0.0001355256827082485,\n",
       "  -0.0011872857576236129,\n",
       "  -0.0016858766321092844,\n",
       "  -0.0028222817927598953,\n",
       "  -0.003666942473500967,\n",
       "  -0.0033234250731766224,\n",
       "  -0.0022821053862571716,\n",
       "  -0.0010346679482609034,\n",
       "  -0.0014158709673210979,\n",
       "  -0.003421393223106861,\n",
       "  -0.0048864008858799934,\n",
       "  -0.005201362539082766,\n",
       "  -0.004031051881611347,\n",
       "  -0.0026276991702616215,\n",
       "  -0.0008561410359106958,\n",
       "  0.0011084980797022581,\n",
       "  0.0013371603563427925,\n",
       "  0.0005927217425778508,\n",
       "  -0.0006846385658718646,\n",
       "  -0.0016543497331440449,\n",
       "  -0.0014591955114156008,\n",
       "  5.5656688346061856e-05,\n",
       "  0.0030013886280357838,\n",
       "  0.005760739091783762,\n",
       "  0.006784629542380571,\n",
       "  0.006534581538289785,\n",
       "  0.005068667232990265,\n",
       "  0.0025274595245718956,\n",
       "  0.0001878336479421705,\n",
       "  -0.000713110901415348,\n",
       "  -9.112974657909945e-05,\n",
       "  0.0007145970594137907,\n",
       "  0.001915477798320353,\n",
       "  0.002468785271048546,\n",
       "  0.0016977923223748803,\n",
       "  0.0004901922075077891,\n",
       "  -0.0010721872095018625,\n",
       "  -0.0015928867505863309,\n",
       "  -0.00047805250505916774,\n",
       "  0.0013999150833114982,\n",
       "  0.002839829307049513,\n",
       "  0.0031520018819719553,\n",
       "  0.002229195786640048,\n",
       "  0.0005427994765341282,\n",
       "  -0.0010768992360681295,\n",
       "  -0.0019247260643169284,\n",
       "  -0.001801978563889861,\n",
       "  -0.0011330986162647605,\n",
       "  -0.0003087321820203215,\n",
       "  0.0005581140867434442,\n",
       "  0.0010014777071774006,\n",
       "  0.0005152930389158428,\n",
       "  -0.0004964703693985939,\n",
       "  -0.0014094519428908825,\n",
       "  -0.0013692759675905108,\n",
       "  -0.0012362621491774917,\n",
       "  -0.0009925656486302614,\n",
       "  -0.0006596518214792013,\n",
       "  -0.001023380900733173,\n",
       "  -0.0017347888788208365,\n",
       "  -0.003205768298357725,\n",
       "  -0.0038944161497056484,\n",
       "  -0.003574459347873926,\n",
       "  -0.002754919696599245,\n",
       "  -0.001633145147934556,\n",
       "  -0.000953378330450505,\n",
       "  -0.00044074482866562903,\n",
       "  5.196046913624741e-05,\n",
       "  3.251985617680475e-05,\n",
       "  -0.0011438827496021986,\n",
       "  -0.001601573545485735,\n",
       "  -0.0009247252601198852,\n",
       "  0.0002949016052298248,\n",
       "  0.002078562742099166,\n",
       "  0.0035592596977949142,\n",
       "  0.003848825814202428,\n",
       "  0.002869088901206851,\n",
       "  0.0013957405462861061,\n",
       "  -0.00021700386423617601,\n",
       "  -0.0006707774591632187,\n",
       "  -0.0006763614364899695,\n",
       "  -0.00041300547309219837,\n",
       "  0.0006666137487627566,\n",
       "  0.0013133358443155885,\n",
       "  0.0020552200730890036,\n",
       "  0.0021060972940176725,\n",
       "  0.0012822443386539817,\n",
       "  0.0010551331797614694,\n",
       "  0.0011025278363376856,\n",
       "  0.0016988818533718586,\n",
       "  0.0025903170462697744,\n",
       "  0.0034978282637894154,\n",
       "  0.0043176584877073765,\n",
       "  0.0041000801138579845,\n",
       "  0.003784904722124338,\n",
       "  0.0027870824560523033,\n",
       "  0.0013462577480822802,\n",
       "  0.0007658082759007812,\n",
       "  2.3457039787899703e-05,\n",
       "  -0.0003146371163893491,\n",
       "  6.793296779505908e-05,\n",
       "  0.00038922319072298706,\n",
       "  0.00016582707758061588,\n",
       "  -0.00036537673440761864,\n",
       "  -0.0015166023513302207,\n",
       "  -0.003311613341793418,\n",
       "  -0.004720219410955906,\n",
       "  -0.0054220957681536674,\n",
       "  -0.005339904688298702,\n",
       "  -0.005027832463383675,\n",
       "  -0.004742187913507223,\n",
       "  -0.0041251108050346375,\n",
       "  -0.0026040137745440006,\n",
       "  -0.001292392727918923,\n",
       "  -0.00098990008700639,\n",
       "  -0.00039586343336850405,\n",
       "  0.00048255236470140517,\n",
       "  0.0006377971149049699,\n",
       "  0.0009449717472307384,\n",
       "  0.0014714202843606472,\n",
       "  0.001035899855196476,\n",
       "  0.0010796920396387577,\n",
       "  0.0010527435224503279,\n",
       "  0.0008324915543198586,\n",
       "  0.0011458121007308364,\n",
       "  0.0010172600159421563,\n",
       "  0.000737056543584913,\n",
       "  0.00017185101751238108,\n",
       "  -0.00032539208768866956,\n",
       "  -0.001204676111228764,\n",
       "  -0.0018639516783878207,\n",
       "  -0.0018768375739455223,\n",
       "  -0.0020757149904966354,\n",
       "  -0.0013733160449191928,\n",
       "  -0.00041910697473213077,\n",
       "  6.397080869646743e-05,\n",
       "  0.0003003902093041688,\n",
       "  0.0004848971439059824,\n",
       "  0.0008033552439883351,\n",
       "  0.001197292935103178,\n",
       "  0.0020152523647993803,\n",
       "  0.002967584179714322,\n",
       "  0.00456790579482913,\n",
       "  0.0059594339691102505,\n",
       "  0.006409916561096907,\n",
       "  0.0068392339162528515,\n",
       "  0.005576305091381073,\n",
       "  0.0038861692883074284,\n",
       "  0.0029306532815098763,\n",
       "  0.0013242842396721244,\n",
       "  0.00032178699620999396,\n",
       "  0.00031379793654195964,\n",
       "  0.0011153947561979294,\n",
       "  0.0014800899662077427,\n",
       "  0.0015041737351566553,\n",
       "  0.0013541068183258176,\n",
       "  4.726189217763022e-06,\n",
       "  -0.0007472267025150359,\n",
       "  -0.0011126500321552157,\n",
       "  -0.0011487457668408751,\n",
       "  3.3262713259318843e-05,\n",
       "  0.00091374950716272,\n",
       "  0.00014722393825650215,\n",
       "  -0.0014494765782728791,\n",
       "  -0.0024378865491598845,\n",
       "  -0.0030656554736196995,\n",
       "  -0.0033946558833122253,\n",
       "  -0.0030703353695571423,\n",
       "  -0.002120001707226038,\n",
       "  -0.0011563535081222653,\n",
       "  ...],\n",
       " 'labels': [2321, 1971, 1989, 1042, 2168, 1882, 3650],\n",
       " 'input_length': 2.784}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prepare_dataset_for_batching(batch, processor_obj=None):\n",
    "    # Extract audio data\n",
    "    audio_arrays = [item[\"array\"] for item in batch[\"audio\"]]\n",
    "    sampling_rates = [item[\"sampling_rate\"] for item in batch[\"audio\"]]\n",
    "    sentences = batch[\"sentence\"]  # List of strings\n",
    "\n",
    "    # Process audio inputs (without padding)\n",
    "    model_inputs = processor_obj(\n",
    "        audio_arrays,\n",
    "        sampling_rate=sampling_rates[0],\n",
    "        padding=False,  # Crucial: no padding at this stage\n",
    "        return_tensors=None,  # Get raw lists instead of tensors\n",
    "    )\n",
    "\n",
    "    batch[\"input_values\"] = model_inputs.input_values\n",
    "\n",
    "    # Process text labels (without padding)\n",
    "    # Use tokenizer directly with add_special_tokens=False for CTC\n",
    "    batch[\"labels\"] = processor_obj.tokenizer(\n",
    "        sentences, \n",
    "        add_special_tokens=False,  # No special tokens for CTC\n",
    "        padding=False,  # No padding - handled by collator\n",
    "    ).input_ids\n",
    "\n",
    "    # Calculate audio lengths\n",
    "    batch['input_length'] = [\n",
    "        len(arr) / sr \n",
    "        for arr, sr in zip(audio_arrays, sampling_rates)\n",
    "    ]\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Then call map like this:\n",
    "common_voice_train = common_voice_train.map(\n",
    "    prepare_dataset_for_batching,\n",
    "    #remove_columns=columns_to_remove_train,\n",
    "    num_proc=3, # Can now safely increase this for parallel batch processing\n",
    "    batched=True, # <--- IMPORTANT: Set to True\n",
    "    fn_kwargs={\"processor_obj\": processor}, # Still good practice for num_proc > 1\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "common_voice_train[0]  # Check the first entry to see if it worked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47e2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data collator for CTC with padding and masking\n",
    "@dataclass\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42c4c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:2176: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Metrics and model initialization, feature extractor, and model loading\n",
    "import evaluate\n",
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "# Load the built-in CER metric\n",
    "# cer_metric = load_metric(\"cer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "# def compute_metrics(pred):\n",
    "#     pred_logits = pred.predictions\n",
    "#     pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "#     pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "#     pred_str = processor.batch_decode(pred_ids)\n",
    "#     label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "#     cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "#     return {\"cer\": cer}\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Avoid in-place modification\n",
    "    label_ids = pred.label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    args.model,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "\n",
    "if not args.unfreeze:\n",
    "    model.freeze_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312923cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\westw\\AppData\\Local\\Temp\\ipykernel_25748\\1618125113.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 35524, 16296, 36476, 25264, 32348, 29900, 26460, 35828, 29052, 17460, 36148, 10988, 35252) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Programs\\Spyder\\Lib\\queue.py:179\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    180\u001b[39m \u001b[38;5;28mself\u001b[39m.not_empty.wait(remaining)\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m      2\u001b[39m training_args = TrainingArguments(\n\u001b[32m      3\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./wav2vec2-large-xlsr-cantonese\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     group_by_length=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m   \u001b[38;5;66;03m#  optim=\"adamw_8bit\"\u001b[39;00m\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m trainer = Trainer(\n\u001b[32m     23\u001b[39m     model=model,\n\u001b[32m     24\u001b[39m     data_collator=data_collator,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     tokenizer=processor.feature_extractor,\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2508\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2506\u001b[39m update_step += \u001b[32m1\u001b[39m\n\u001b[32m   2507\u001b[39m num_batches = args.gradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step != (total_updates - \u001b[32m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[32m-> \u001b[39m\u001b[32m2508\u001b[39m batch_samples, num_items_in_batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[32m   2510\u001b[39m     step += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:5224\u001b[39m, in \u001b[36mTrainer.get_batch_samples\u001b[39m\u001b[34m(self, epoch_iterator, num_batches, device)\u001b[39m\n\u001b[32m   5222\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[32m   5223\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m5224\u001b[39m         batch_samples += [\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m   5225\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   5226\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\accelerate\\data_loader.py:566\u001b[39m, in \u001b[36mDataLoaderShard.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     current_batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1410\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1264\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1263\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1265\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1266\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 35524, 16296, 36476, 25264, 32348, 29900, 26460, 35828, 29052, 17460, 36148, 10988, 35252) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Set training arguments and initialize the Trainer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wav2vec2-large-xlsr-cantonese\",\n",
    "    group_by_length=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=400,\n",
    "    num_train_epochs=40,\n",
    "    fp16=True,\n",
    "    fp16_backend=\"amp\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=400,\n",
    "    learning_rate=args.lr,\n",
    "    warmup_steps=100,\n",
    "    save_steps=2376,\n",
    "    save_total_limit=3,\n",
    "    dataloader_num_workers=20,\n",
    "  #  optim=\"adamw_8bit\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
