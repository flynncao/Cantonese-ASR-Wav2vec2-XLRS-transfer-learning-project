{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4379dba",
   "metadata": {},
   "source": [
    "\n",
    "A data collator class for CTC (Connectionist Temporal Classification) with padding functionality.\n",
    "This class handles the batching and padding of input features and labels for wav2vec2 model training.\n",
    "It processes audio features and their corresponding transcription labels, ensuring proper padding\n",
    "and tensor conversion.\n",
    "Args:\n",
    "\tprocessor (Wav2Vec2Processor): The wav2vec2 processor for handling inputs and labels\n",
    "\tpadding (Union[bool, str]): The padding strategy to use. Defaults to True.\n",
    "\tmax_length (Optional[int]): Maximum length for input features padding. Defaults to None.\n",
    "\tmax_length_labels (Optional[int]): Maximum length for labels padding. Defaults to None.\n",
    "\tpad_to_multiple_of (Optional[int]): Pad input features to be multiple of this value. Defaults to None.\n",
    "\tpad_to_multiple_of_labels (Optional[int]): Pad labels to be multiple of this value. Defaults to None.\n",
    "Methods:\n",
    "\t__call__(features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "\t\tProcesses a batch of features to create padded tensors suitable for model training.\n",
    "\t\tArgs:\n",
    "\t\t\tfeatures: List of dictionaries containing input values and labels\n",
    "\t\tReturns:\n",
    "\t\t\tDict containing padded input tensors and processed labels with -100 for padding tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c5bd447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "import transformers\n",
    "from datasets import ClassLabel, load_dataset, load_metric, load_from_disk\n",
    "from transformers import (Trainer, TrainingArguments, Wav2Vec2CTCTokenizer,\n",
    "                          Wav2Vec2FeatureExtractor, Wav2Vec2ForCTC,\n",
    "                          Wav2Vec2Processor)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b265da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args: Namespace(model='facebook/wav2vec2-large-xlsr-53', unfreeze=False, lr=0.0003, warmup=500, fff='c:\\\\Users\\\\westw\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v310886c7194b81b675aae211578db6de1832a187f.json')\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser() \n",
    "parser.add_argument('--model', type=str, default=\"facebook/wav2vec2-large-xlsr-53\")\n",
    "parser.add_argument('--unfreeze', action='store_true')\n",
    "parser.add_argument('--lr', type=float, default=3e-4)\n",
    "parser.add_argument('--warmup', type=float, default=500)\n",
    "parser.add_argument('-f', '--fff', help=\"dummy argument to avoid error in Jupyter\", default=\"dummy_value\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "print(f\"args: {args}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc33a3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\datasets\\load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 从本地磁盘加载数据集 Load Cantonese language only \n",
    "common_voice_train = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"zh-HK\", split=\"train\")\n",
    "common_voice_test = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"zh-HK\", split=\"test\")\n",
    "\n",
    "unused_cols = [\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"]\n",
    "common_voice_train = common_voice_train.remove_columns(unused_cols)\n",
    "common_voice_test = common_voice_test.remove_columns(unused_cols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edf162f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence', 'variant'],\n",
       "    num_rows: 5593\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test if the duration column is added\n",
    "print(common_voice_train[0])\n",
    "print(common_voice_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6719d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/8425 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8425/8425 [00:00<00:00, 181566.47 examples/s]\n",
      "Map: 100%|██████████| 5593/5593 [00:00<00:00, 275056.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "\n",
    "chars_to_ignore_regex = '[\\丶\\,\\?\\.\\!\\-\\;\\:\"\\“\\%\\‘\\”\\�\\．\\⋯\\！\\－\\：\\–\\。\\》\\,\\）\\,\\？\\；\\～\\~\\…\\︰\\，\\（\\」\\‧\\《\\﹔\\、\\—\\／\\,\\「\\﹖\\·\\']'\n",
    "\n",
    "import string\n",
    "def remove_special_characters(batch):\n",
    "    sen = re.sub(chars_to_ignore_regex, '', batch[\"sentence\"]).lower() + \" \"\n",
    "    if \"d\" in sen:\n",
    "        if len([c for c in sen if c in string.ascii_lowercase]) == 1:\n",
    "            sen = sen.replace(\"d\", \"啲\")\n",
    "    batch[\"sentence\"] = sen\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)\n",
    "\n",
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"sentence\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "vocab_train = common_voice_train.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_train.column_names,)\n",
    "vocab_test = common_voice_test.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=common_voice_test.column_names,)\n",
    "vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
    "vocab_list = [char for char in vocab_list if not char.isascii()]\n",
    "vocab_list.append(\" \")\n",
    "\n",
    "vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "with open(\"vocab.json\", \"w\") as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c3fcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets and resampling, the modern way\n",
    "from datasets import Audio\n",
    "common_voice_train = common_voice_train.cast_column(\"audio\", Audio(sampling_rate=16000)) # reinterpret this column (\"audio\") as a certain type, with new settings\n",
    "common_voice_test = common_voice_test.cast_column(\"audio\", Audio(sampling_rate=16000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58766f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['path', 'audio', 'sentence', 'variant'],\n",
       "    num_rows: 5593\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d6c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"processor_class\": \"Wav2Vec2Processor\",\n",
       "  \"return_attention_mask\": true,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: Wav2Vec2CTCTokenizer(name_or_path='', vocab_size=3653, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'pad_token': '[PAD]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t3651: AddedToken(\"[UNK]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t3652: AddedToken(\"[PAD]\", rstrip=True, lstrip=True, single_word=False, normalized=False, special=False),\n",
       "\t3653: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3654: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"Wav2Vec2Processor\"\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Define the prepare_dataset function (like your Whisper one) ---\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\"./vocab.json\", unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True,)\n",
    "\n",
    "processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "processor.save_pretrained(\"./wav2vec2-large-xlsr-cantonese\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ca1c53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=5): 100%|██████████| 8425/8425 [03:10<00:00, 44.32 examples/s]\n",
      "Map (num_proc=5):   0%|          | 0/5593 [00:00<?, ? examples/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def prepare_dataset_for_batching(batch, processor_obj=None):\n",
    "    # Extract audio data\n",
    "    audio_arrays = [item[\"array\"] for item in batch[\"audio\"]]\n",
    "    sampling_rates = [item[\"sampling_rate\"] for item in batch[\"audio\"]]\n",
    "    sentences = batch[\"sentence\"]  # List of strings\n",
    "\n",
    "    # Process audio inputs (without padding)\n",
    "    model_inputs = processor_obj(\n",
    "        audio_arrays,\n",
    "        sampling_rate=sampling_rates[0],\n",
    "        padding=False,  # Crucial: no padding at this stage\n",
    "        return_tensors=None,  # Get raw lists instead of tensors\n",
    "    )\n",
    "\n",
    "    batch[\"input_values\"] = model_inputs.input_values # input values can be obtained from model_inputs\n",
    "\n",
    "    # Process text labels (without padding)\n",
    "    # Use tokenizer directly with add_special_tokens=False for CTC\n",
    "    batch[\"labels\"] = processor_obj.tokenizer(\n",
    "        sentences, \n",
    "        add_special_tokens=False,  # No special tokens for CTC\n",
    "        padding=False,  # No padding - handled by collator\n",
    "    ).input_ids\n",
    "\n",
    "    '''\n",
    "\t\t# Calculate audio durations in seconds\n",
    "\t\t# audio_arrays: List of audio arrays where each array represents audio samples\n",
    "\t\t# sampling_rates: Corresponding sampling rates for each audio array\n",
    "\t\t# Result is stored in batch['input_length'] as list of durations\n",
    "\t\t'''   \n",
    "\t\t\n",
    "    batch['input_length'] = [\n",
    "        len(arr) / sr \n",
    "        for arr, sr in zip(audio_arrays, sampling_rates)\n",
    "    ]\n",
    "\n",
    "    return batch\n",
    "\n",
    "# Then call map like this:\n",
    "common_voice_train = common_voice_train.map(\n",
    "    prepare_dataset_for_batching,\n",
    "    #remove_columns=columns_to_remove_train,\n",
    "    num_proc=5, # Can now safely increase this for parallel batch processing\n",
    "    batched=True, # <--- IMPORTANT: Set to True\n",
    "    fn_kwargs={\"processor_obj\": processor}, # Still good practice for num_proc > 1\n",
    "    load_from_cache_file=True\n",
    ")\n",
    "\n",
    "common_voice_test = common_voice_test.map(\n",
    "    prepare_dataset_for_batching,\n",
    "    #remove_columns=columns_to_remove_train,\n",
    "    num_proc=5, # Can now safely increase this for parallel batch processing\n",
    "    batched=True, # <--- IMPORTANT: Set to True\n",
    "    fn_kwargs={\"processor_obj\": processor}, # Still good practice for num_proc > 1\n",
    "    load_from_cache_file=True\n",
    ")\n",
    "\n",
    "\n",
    "# Then remove unused columns from BOTH datasets\n",
    "columns_to_remove = [\"path\", \"audio\", \"sentence\", \"variant\"]\n",
    "common_voice_train = common_voice_train.remove_columns(columns_to_remove)\n",
    "common_voice_test = common_voice_test.remove_columns(columns_to_remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47e2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data collator for CTC with padding and masking\n",
    "@dataclass\n",
    "\n",
    "class DataCollatorCTCWithPadding:\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length_labels,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4c8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-xlsr-53 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:2176: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5. Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Metrics and model initialization, feature extractor, and model loading\n",
    "import evaluate\n",
    "data_collator = DataCollatorCTCWithPadding(\n",
    "    processor=processor,\n",
    "    padding=True,\n",
    "    max_length=int(16_000 * 15),     # cap at 15s\n",
    "    max_length_labels=512,\n",
    "    pad_to_multiple_of=16,\n",
    "    pad_to_multiple_of_labels=8,\n",
    ")\n",
    "# Load the built-in CER metric\n",
    "# cer_metric = load_metric(\"cer\")\n",
    "cer_metric = evaluate.load(\"cer\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    # Avoid in-place modification\n",
    "    label_ids = pred.label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.batch_decode(label_ids, group_tokens=False, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    return {\"cer\": cer}\n",
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    args.model,\n",
    "    attention_dropout=0.1,\n",
    "    hidden_dropout=0.1,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.1,\n",
    "    gradient_checkpointing=True,\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "if not args.unfreeze:\n",
    "    model.freeze_feature_extractor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd484c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312923cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1724: FutureWarning: `fp16_backend` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `half_precision_backend` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\westw\\AppData\\Local\\Temp\\ipykernel_27876\\3981024693.py:58: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "f:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:174: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='401' max='5270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 401/5270 04:41 < 57:10, 1.42 it/s, Epoch 0.76/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='229' max='700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [229/700 15:40 < 32:23, 0.24 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 13.23 GiB. GPU 0 has a total capacity of 15.99 GiB of which 0 bytes is free. Of the allocated memory 15.01 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     58\u001b[39m trainer = Trainer(\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Model and device\u001b[39;00m\n\u001b[32m     60\u001b[39m     model=model.to(device),\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m     tokenizer=processor.feature_extractor,\n\u001b[32m     77\u001b[39m )\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2620\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2618\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2619\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2620\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2621\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\n\u001b[32m   2622\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2624\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3093\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[39m\n\u001b[32m   3091\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3093\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3094\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3096\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3047\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3046\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3047\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3048\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3050\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4136\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4133\u001b[39m start_time = time.time()\n\u001b[32m   4135\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4136\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4137\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4139\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4140\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4144\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4146\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4357\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4355\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.gather_function(logits)\n\u001b[32m   4356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.batch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description == \u001b[33m\"\u001b[39m\u001b[33mPrediction\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m4357\u001b[39m         \u001b[43mall_preds\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4359\u001b[39m     labels = \u001b[38;5;28mself\u001b[39m.gather_function(labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:316\u001b[39m, in \u001b[36mEvalLoopContainer.add\u001b[39m\u001b[34m(self, tensors)\u001b[39m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors = tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.do_nested_concat:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors = \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28mself\u001b[39m.tensors.append(tensors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:130\u001b[39m, in \u001b[36mnested_concat\u001b[39m\u001b[34m(tensors, new_tensors, padding_index)\u001b[39m\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index=padding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[32m    133\u001b[39m         {k: nested_concat(t, new_tensors[k], padding_index=padding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors.items()}\n\u001b[32m    134\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hf\\whisper-larger-v3-turbo-playground\\.venv\\Lib\\site-packages\\transformers\\trainer_pt_utils.py:94\u001b[39m, in \u001b[36mtorch_pad_and_concatenate\u001b[39m\u001b[34m(tensor1, tensor2, padding_index)\u001b[39m\n\u001b[32m     91\u001b[39m new_shape = (tensor1.shape[\u001b[32m0\u001b[39m] + tensor2.shape[\u001b[32m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1.shape[\u001b[32m1\u001b[39m], tensor2.shape[\u001b[32m1\u001b[39m])) + tensor1.shape[\u001b[32m2\u001b[39m:]\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# Now let's fill the result tensor\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m result = \u001b[43mtensor1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnew_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m result[: tensor1.shape[\u001b[32m0\u001b[39m], : tensor1.shape[\u001b[32m1\u001b[39m]] = tensor1\n\u001b[32m     96\u001b[39m result[tensor1.shape[\u001b[32m0\u001b[39m] :, : tensor2.shape[\u001b[32m1\u001b[39m]] = tensor2\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 13.23 GiB. GPU 0 has a total capacity of 15.99 GiB of which 0 bytes is free. Of the allocated memory 15.01 GiB is allocated by PyTorch, and 2.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Configure PyTorch Dynamo to suppress errors during optimization\n",
    "import torch._dynamo\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Training arguments configuration\n",
    "training_args = TrainingArguments(\n",
    "    # Output directory for model checkpoints and logs\n",
    "    output_dir=\"./wav2vec2-large-xlsr-cantonese\",\n",
    "    \n",
    "    # Group sequences of similar lengths together to improve efficiency\n",
    "    group_by_length=True,\n",
    "    \n",
    "    # Batch size per device for training\n",
    "    per_device_train_batch_size=16,\n",
    "    \n",
    "    # Number of gradient accumulation steps\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    \n",
    "    # Evaluation strategy and frequency \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=400,\n",
    "    \n",
    "    # Number of training epochs\n",
    "    num_train_epochs=10,\n",
    "    \n",
    "    # Mixed precision training settings\n",
    "    fp16=True,\n",
    "    fp16_backend=\"amp\",\n",
    "    fp16_full_eval=True,\n",
    "    \n",
    "    # Logging configuration\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=400,\n",
    "    \n",
    "    # Learning rate and warmup\n",
    "    learning_rate=args.lr,\n",
    "    warmup_steps=500,\n",
    "    \n",
    "    # Model checkpointing\n",
    "    save_steps=2376,\n",
    "    save_total_limit=3,\n",
    "    \n",
    "    # Data loading configuration\n",
    "    dataloader_num_workers=0,\n",
    "    \n",
    "    # Optimizer settings\n",
    "    optim=\"adamw_8bit\",\n",
    "    \n",
    "    # Misc settings\n",
    "    remove_unused_columns=False,\n",
    "    torch_compile=False, # support not well on some models, so set to False for now\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    # Model and device\n",
    "    model=model.to(device),\n",
    "    \n",
    "    # Data collator for batching\n",
    "    data_collator=data_collator,\n",
    "    \n",
    "    # Training configuration\n",
    "    args=training_args,\n",
    "    \n",
    "    # Metrics computation function \n",
    "    compute_metrics=compute_metrics,\n",
    "    \n",
    "    # Training and evaluation datasets\n",
    "    train_dataset=common_voice_train,\n",
    "    eval_dataset=common_voice_test,\n",
    "    \n",
    "    # Tokenizer/feature extractor\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
